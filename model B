# Import modules for the model to run
import copy
import random
import time
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.models import resnet18, ResNet18_Weights
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    roc_auc_score,
)
#function returns mean loss, does not backpropagate gradients, batchNorm or perform dropout
@torch.no_grad() # disables Pytorch gradient tracking
def eval_loss(model, loader, criterion, device): #runs validation data
    model.eval()
    running = 0.0
    n = 0
    for x, y in loader:
        x = x.to(device)
        y = y.to(device).float()
        logits = model(x).squeeze(1)
        loss = criterion(logits, y)
        bs = x.size(0)
        running += loss.item() * bs
        n += bs
    return running / max(n, 1)

#to make results repeatebale or reproducibility
def seed_everything(seed: int = 20):
    random.seed(seed) #controls  randomness from random()
    np.random.seed(seed)#controlls  randomness from Numpy
    torch.manual_seed(seed)# controlls randomness of torch on CPU

#class takes up images with labels,shapes them, converts to PIL, applies transforms
class NumpyToTorchDataset(Dataset):
    def __init__(self, images_np, labels_np, transform=None):
        self.images = images_np
        self.labels = labels_np.reshape(-1).astype(np.int64)
        self.transform = transform
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        img = self.images[idx]
    #gives gray scale shape
        if img.ndim == 3 and img.shape[-1] == 1:
            img = img.squeeze(-1)
    # Convert to PIL, and applies  transforms
        img_pil = transforms.ToPILImage()(img)
        if self.transform is not None:
            x = self.transform(img_pil)
        else:
            x = transforms.ToTensor()(img_pil)
        y = self.labels[idx]
        return x, y

#binary metric definitions
def binary_metrics(y_true, probs, threshold=0.35):
    y_pred = (probs >= threshold).astype(int)
    acc = accuracy_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()
    sens = tp / (tp + fn + 1e-11)
    spec = tn / (tn + fp + 1e-11)
    auc = float("nan")
    if len(np.unique(y_true)) == 2:
        auc = roc_auc_score(y_true, probs)
    return acc, sens, spec, auc, cm

@torch.no_grad()
def eval_auc(model, loader, device):
    y_true, probs = predict_probability(model, loader, device)
    _, _, _, auc, _ = binary_metrics(y_true, probs, threshold=0.5)
    return auc

#function to train and evaluate , optimize model over epoch, returns mean loss
def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running = 0.0
    n = 0
    for x, y in loader:
        x = x.to(device)
        y = y.to(device).float()
        optimizer.zero_grad()
        logits = model(x).squeeze(1) #forward pass
        loss = criterion(logits, y)#computes loss
        loss.backward() # computes gradients
        optimizer.step() #updates trainable parameters
        bs = x.size(0)
        running += loss.item() * bs
        n += bs
    return running / max(n, 1)


#metric definitions, returns y-true labels, returns probabilities  after sigmod (logits)
# no gradients , evaluation only, loops over bacthes
@torch.no_grad()
def predict_probability(model, loader, device):
    model.eval()
    all_probs = []
    all_y = []
    for x, y in loader:
        x = x.to(device)
        logits = model(x).squeeze(1)
        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.append(probs)
        all_y.append(y.numpy())
    probs = np.concatenate(all_probs, axis=0)
    y_true = np.concatenate(all_y, axis=0).astype(int)
    return y_true, probs



def main():
    seed_everything(28)
    device = torch.device( "cpu") # runs on CPU powered machine
    import medmnist
    from medmnist import INFO
    data_flag = "breastmnist" # pick breastminst from medmnist
    info = INFO[data_flag]
    DataClass = getattr(medmnist, info["python_class"])
    train_ds_raw = DataClass(split="train", download=True) # train dataset and catches
    test_ds_raw  = DataClass(split="test", download=True) #test dataset and catches

    X_train_full = train_ds_raw.imgs # extract raw arrays (N,28,28)
    y_train_full = train_ds_raw.labels.reshape(-1).astype(int) # extract raw arrays (N,28,28), flattens data
    X_test = test_ds_raw.imgs
    y_test = test_ds_raw.labels.reshape(-1).astype(int)
    pp = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # 15% as validation split
    train_idx, val_idx = next(pp.split(X_train_full, y_train_full))
    X_train = X_train_full[train_idx]
    y_train = y_train_full[train_idx]
    X_val   = X_train_full[val_idx]
    y_val   = y_train_full[val_idx]

#image augumentation and preprocessing using Torchvision transform, converted to tensors , normalised
    img_size = 128
    allow_hflip = True # horinzontal flips allowed

    train_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.RandomApply([
            transforms.RandomAffine(
                degrees=10,
                translate=(0.10, 0.08),
                scale=(0.90, 1.00),
            )
        ], p=0.8),
        transforms.RandomHorizontalFlip(p=0.4 if allow_hflip else 0.0),
        transforms.ToTensor(),
        transforms.Lambda(lambda t: t.repeat(3, 1, 1)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])
#preprocessing without random augumenation, deterministic transfroms for validation set
    eval_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Lambda(lambda t: t.repeat(3, 1, 1)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])

#creates Pytorch datasets, samples per batch variable
    train_dataset = NumpyToTorchDataset(X_train, y_train, transform=train_transform)
    val_dataset   = NumpyToTorchDataset(X_val,   y_val,   transform=eval_transform)
    test_dataset  = NumpyToTorchDataset(X_test,  y_test,  transform=eval_transform)

#computes class imbalances
    n_pos = int((y_train == 1).sum())
    n_neg = int((y_train == 0).sum())
    if n_pos == 0:
        raise ValueError("No positive  malignant samples in training dataset.")
    pos_weight = torch.tensor([n_neg / max(n_pos, 1)], dtype=torch.float32, device=device)
#data loaders with variable batch size
    batch_size = 30
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)
    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)

    #import  Model ResNet-18
    weights = ResNet18_Weights.IMAGENET1K_V1 #selects pretrained weights on imagnet
    model = resnet18(weights=weights) #constructs the model and loads weights to get the resnet18 backbone
    in_features = model.fc.in_features # feature vector =512 for resnet18
    model.fc = nn.Sequential(
        nn.Dropout(p=0.2), #drops 20% during traing to reduce overfiting
        nn.Linear(in_features, 1) # outputs a logit
    )
    model = model.to(device)

#tarnsfer learning, freeze backbone and train the fc /head
    for name, p in model.named_parameters():
        p.requires_grad = name.startswith("fc.")

#loss is defined
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    history = {"train_loss": [], "val_loss": [], "val_auc": []} #training history , stores weights of best model
    best_auc = -1.0
    best_state = None

    def update_best(model_):
        nonlocal best_auc, best_state
        auc = eval_auc(model_, val_loader, device) # runs function eval_auc
        if np.isfinite(auc) and auc > best_auc:
            best_auc = auc
            best_state = copy.deepcopy(model_.state_dict())
        return auc
#optimizer updates tarinable parameters in the fc head, hyperparamters ( lr, weight_decay as regularasation knob)
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                            lr=1e-3, weight_decay=1e-4)

    max_epochs_stage2 = 15  # head is trained for so many epochs

#loop over epochs
    for epoch2 in range(1, max_epochs_stage2 + 1):
        t0 = time.time()
        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, device) #evalautes loss on train set
        va_loss = eval_loss(model, val_loader, criterion, device) # evaluates validation loss
        va_auc = update_best(model)

        history["train_loss"].append(tr_loss)
        history["val_loss"].append(va_loss)
        history["val_auc"].append(va_auc)
#ploting metrics
        e = len(history["train_loss"])
        print(f"Epoch {e:02d} | train_loss={tr_loss:.3f} | val_loss={va_loss:.3f} "
              f"| val_auc={va_auc:.3f} | best_auc={best_auc:.3f} | {time.time()-t0:.1f}s")
    if best_state is not None:
        model.load_state_dict(best_state)
    else:
        print("\nWARNING")

    plt.figure()
    plt.plot(history["train_loss"], label="train_loss")
    plt.plot(history["val_loss"], label="val_loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Epoch vs Loss")
    plt.legend()
    plt.tight_layout()
    plt.show()
#prints model architecture , ResNet18 backbone and FC head
    print(model)

#runs model on test set, collects labels and predictetd probabilities
    true_labels, pred_prob = predict_probability(model, test_loader, device)
#computes binary classsifcatio metrics at variable threshold, lower threshold =higher sensitivity,less missed malignsts
    accu, sens, speci, auc, bm = binary_metrics(true_labels, pred_prob, threshold=0.35)
#prints computed accu =TP+Tn/N, sens=computed positives predicted, speci=computed negatives rejected
    print(f"Accuracy     : {accu:.3f}")
    print(f"Sensitivity  : {sens:.3f}  (Recall for malignant=1)")
    print(f"Specificity  : {speci:.3f}  (True negative rate for benign=0)")
    print(f"ROC-AUC      : {auc:.3f}")
    print("Confusion Matrix [[TN, FP],[FN, TP]]:")
    print(bm)

if __name__ == "__main__":
    main()
